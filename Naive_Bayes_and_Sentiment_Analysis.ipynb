{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment Analysis\n",
    "\n",
    "TEM501 - Text Mining\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This document will instruct you how to perform sentiment classifiation by using Naive Bayes algorithm. We use [scikit-learn](http://scikit-learn.org/) to perform the task. After reading this document, students can understand how to perform following tasks.\n",
    "\n",
    "- Read sentiment data from a text file\n",
    "- Split data into a train/test file\n",
    "- Feature extraction: convert a sentence into a feature vector\n",
    "- Train a Naive Bayes model on the training data\n",
    "- Evaluate on the test file\n",
    "- Perform k-fold cross validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use the [sentence polarity dataset v1.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt) from [Moview Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/) of Bo Pang v√† Lillian Lee. We can see the data file in [./data/sentiment.txt](./data/sentiment.txt).\n",
    "\n",
    "Each line in the file is a review which was already tokenized into words. Each review has a label (+1 for positive review and -1 for negative review).\n",
    "\n",
    "\n",
    "## Loading data\n",
    "\n",
    "We will load the data into a list of tuples $(d, c)$ in which $d$ denote a document and $c$ denotes the label of the document. We define the function `load_data` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            match = re.search(r\"(\\+1|-1)[\\s\\t]+(.+)$\", line)  # match the line +1 ...\n",
    "            if match:\n",
    "                lb = match.group(1)\n",
    "                sentence = match.group(2)\n",
    "                if sentence == \"\":\n",
    "                    continue\n",
    "                data.append((sentence,lb))\n",
    "    return data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above function to load sentiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Loaded 10662 examples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./data/sentiment.txt\"\n",
    "data = load_data(DATA_PATH)\n",
    "\n",
    "print(\"# Loaded {} examples\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to know how many positive and negative reviews in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label +1: 5331\n",
      "Label -1: 5331\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "docs, labels = zip(*data)\n",
    "counter = Counter(labels)\n",
    "for c in sorted(counter.keys()):\n",
    "    print(\"Label {}: {}\".format(c, counter[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset in balanced. There are 5331 positive reviews and 5331 negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training/test data\n",
    "\n",
    "Now we would like to randomly split the data into training and test data. We will use function `train_test_split` in the module `sklearn.model_selection`. Please refer to [http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) for more details about the function. We would like to use 80% of data for training and 20% of data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews: 8529\n",
      "Test reviews: 2133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_data(DATA_PATH)\n",
    "docs, labels = zip(*data)\n",
    "\n",
    "train_docs, test_docs, train_labels, test_labels = train_test_split(docs, labels,\n",
    "                                                                   test_size=0.2,\n",
    "                                                                   random_state=1337)\n",
    "print(\"Training reviews: {}\".format(len(train_docs)))\n",
    "print(\"Test reviews: {}\".format(len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Now we convert a sentence into a feature vector. We just use BoW (bag-of-words) in each review. In this section, we will implement by ourselves. In next section, we will learn how to use scikit-learn for feature extraction.\n",
    "\n",
    "We define a feature function as follows. Input of the function is a sentence and output is a feature vector. We will remove stopwords and punctuations in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def is_punct(word):\n",
    "    if re.search(r\"^[!\\\"!\\\",\\.:;%&]*$\", word):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def feature_vec(doc):\n",
    "    vec = dict()\n",
    "    for word in doc.split():\n",
    "        if word in eng_stop_words or is_punct(word):\n",
    "            continue\n",
    "        vec[word.lower()] = 1.0\n",
    "    return vec   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to apply the feature function for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thoughtful': 1.0, 'provocative': 1.0, 'insistently': 1.0, 'humanizing': 1.0, 'film': 1.0}\n"
     ]
    }
   ],
   "source": [
    "vec = feature_vec(\"a thoughtful , provocative , insistently humanizing film .\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting data into matrix using DictVectorizer\n",
    "\n",
    "`DictVectorizer` transforms lists of feature-value mappings to vectors. See [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) for more details.\n",
    "\n",
    "We need to transform data (feature mapping) into numeric vectors so that machine learning algorithms can use them as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = [ feature_vec(d) for d in train_docs ]\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the the matrix `X_train`. Scikit-learn store data in sparse matrix data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8529x18887 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 91679 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the vector representation of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6532)\t1.0\n",
      "  (0, 8314)\t1.0\n",
      "  (0, 13102)\t1.0\n",
      "  (0, 16883)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(feature_vec(\"a thoughtful , provocative , insistently humanizing film .\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "We now will train a Naive Bayes model on the training data. Since we use binarize features, so we can use `BernoulliNB` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB(alpha=0.5, binarize=0.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(alpha=0.5)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.5, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the model to predict the label for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ['+1']\n"
     ]
    }
   ],
   "source": [
    "example = \"a thoughtful , provocative , insistently humanizing film .\"\n",
    "test_x = vectorizer.transform(feature_vec(example))\n",
    "print(\"Predicted class: {}\".format(clf.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test set\n",
    "\n",
    "We now use the test data to evaluate the trained model. In the first step, we need to transform the test data into a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = [ feature_vec(d) for d in test_docs ]\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict labels for reviews in the test data by calling `predict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we calculate the accuracy, precision, recall, f1 score by using `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Test accuracy: 0.7515236755743084\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_labels, test_preds)\n",
    "print(\"# Test accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to know precision, recall, f1 score for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: +1\n",
      " Precision: 0.7423954372623575\n",
      " Recall: 0.7509615384615385\n",
      " F1: 0.7466539196940728\n",
      "Label: -1\n",
      " Precision: 0.7604070305272895\n",
      " Recall: 0.7520585544373285\n",
      " F1: 0.7562097516099356\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for label in [\"+1\", \"-1\"]:\n",
    "    p = metrics.precision_score(test_labels, test_preds, average=\"binary\", pos_label=label)\n",
    "    r = metrics.recall_score(test_labels, test_preds, average=\"binary\", pos_label=label)\n",
    "    f1 = metrics.f1_score(test_labels, test_preds, average=\"binary\", pos_label=label)\n",
    "    print(\"Label: {}\".format(label))\n",
    "    print(\" Precision: {}\".format(p))\n",
    "    print(\" Recall: {}\".format(r))\n",
    "    print(\" F1: {}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[781, 259],\n",
       "       [271, 822]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_labels, test_preds, labels=[\"+1\", \"-1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation\n",
    "\n",
    "Now we instruct you to perform k-fold cross validation. We just use the training data for k-fold cross validation, because in general, the test-data is unseen to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold accuracy scores: [0.7644991212653779, 0.7819460726846424, 0.7608440797186401, 0.7589442815249267, 0.7659824046920821]\n",
      "Average score: 0.7664431919771338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "train_docs, train_labels = np.asarray(train_docs), np.asarray(train_labels)\n",
    "\n",
    "n_splits = 5\n",
    "scores = []\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1337)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_docs, train_labels), start=1):\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    vectorizer = DictVectorizer()\n",
    "    \n",
    "    x_train, x_test = train_docs[train_index], train_docs[test_index]\n",
    "    y_train, y_test = train_labels[train_index], train_labels[test_index]\n",
    "    \n",
    "    x_train = [ feature_vec(d) for d in x_train ]\n",
    "    x_train = vectorizer.fit_transform(x_train)\n",
    "    \n",
    "    x_test = [ feature_vec(d) for d in x_test ]\n",
    "    x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf.fit(x_train, y_train)\n",
    "    y_preds = clf.predict(x_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_preds)\n",
    "    scores.append(accuracy)\n",
    "\n",
    "print(\"{}-fold accuracy scores: {}\".format(n_splits, scores))\n",
    "print(\"Average score: {}\".format(np.mean(scores)))   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try to use different alpha values in `BernoulliNB` ([http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)).\n",
    "2. Add bi-gram features and run experiments again.\n",
    "3. Deal with negatation in the sentiment data. You can simply do that by convert phrase \"dont like\" into \"dont NOT_like\"\n",
    "3. Try [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) instead of BernoulliNB and see the difference of the system performance.\n",
    "4. Using feature extraction module of scikit-learn for feature extraction phase. You may want to look at the tutorial [http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) to see how to use feature extraction modules in scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
