{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Applications in Text Classification and Text Clustering\n",
    "\n",
    "Copyright by Pham Quang Nhat Minh (FPT Technology Research Institute (FTRI) - FPT University)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we introduce how to use LDA libraries to estimate topic models on data and how to use topic distributions as representations of documents in text classification and text clustering.\n",
    "\n",
    "## Topic Modeling Overview\n",
    "\n",
    "**Acknowledgement**: Content of this section is credited by [Dr. Le Hong Phuong](http://mim.hus.vnu.edu.vn/phuonglh/).\n",
    "\n",
    "A topic model is a type of statistical model for discovering the abstract topics that occur in a collection of documents. Each topic can be viewed as a distribution of words (a unigram language model). Intuitively, given that a\n",
    "document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ```dog``` and ```bone``` will appear more often in documents about ```dogs```, ```cat``` and ```meow``` will appear in documents about cats, and the and is will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each documentâ€™s balance of topics is.\n",
    "\n",
    "Topic modelling is widely used in text mining and has many applications. In the lecture, you have learned some topic models, including latent semantic indexing (LSI), probabilistic latent semantic analysis (PLSA) and latent\n",
    "Dirichlet allocation (LDA).\n",
    "\n",
    "## Topic Modeling Toolkits\n",
    "\n",
    "There are many implementations of LDA for topic modeling. In the tutorial, we try ```lda-c``` of David Blei and ```gensim``` library.\n",
    "\n",
    "For a longer list of LDA implementation see the article on Wikipedia: [https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "## Get Your Hands Dirty\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this section, we used the sample data that contains 2246 documents from the Associated Press on the homepage of Proff David Blei. Download the data from [http://www.cs.princeton.edu/~blei/lda-c/ap.tgz](http://www.cs.princeton.edu/~blei/lda-c/ap.tgz). On \\*nix environment, you may use the tool wget. After downloading, you need to uncompress the data into the current directory.\n",
    "\n",
    "```\n",
    "wget http://www.cs.princeton.edu/~blei/lda-c/ap.tgz\n",
    "tar xvfz ap.tgz\n",
    "```\n",
    "\n",
    "The data file ```ap/ap.dat``` was converted into LDA format from original documents. The data format is as follows.\n",
    "\n",
    "```\n",
    "[M] [term_1]:[count] [term_2]:[count] ...  [term_N]:[count]\n",
    "```\n",
    "\n",
    "where [M] is the number of unique terms in the document, and the [count] associated with each term is how many times that term appeared in the document. Note that [term_1] is an integer which indexes the term; it is not a string.\n",
    "\n",
    "For more information, you may want to read the instruction in [```readme.txt```](http://www.cs.princeton.edu/~blei/lda-c/readme.txt) within the lda-c-dist directory.\n",
    "\n",
    "### LDA-C\n",
    "\n",
    "lda-c is a C implementation of variational EM for latent Dirichlet allocation (LDA), a topic model for text or other discrete data. LDA is fully described in [Blei et al. (2003)](http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf).\n",
    "\n",
    "You need to download the source code, uncompress the tgz file, and compile the tool before using it.\n",
    "\n",
    "```\n",
    "wget http://www.cs.princeton.edu/~blei/lda-c/lda-c-dist.tgz\n",
    "tar xvfz lda-c-dist.tgz\n",
    "cd lda-c-dist\n",
    "make\n",
    "```\n",
    "\n",
    "### Topic Estimation\n",
    "\n",
    "The syntax for the estimating topics as follows.\n",
    "\n",
    "```\n",
    "lda est [alpha] [k] [settings] [data] [random/seeded/*] [directory]\n",
    "```\n",
    "\n",
    "For the data in the question, we can perform as follows.\n",
    "\n",
    "```\n",
    "./lda-c-dist/lda est 0.001 100 lda-c-dist/settings.txt ap/ap.dat random models/model-001\n",
    "```\n",
    "\n",
    "LDA model will be saved in the directory ```models/model-001```.\n",
    "\n",
    "Will can show a top 20 words for each topic by using the script ```topics.py```.\n",
    "\n",
    "```\n",
    "python2.7 lda-c-dist/topics.py models/model-001/final.beta ap/vocab.txt 20\n",
    "```\n",
    "\n",
    "### Gensim\n",
    "\n",
    "We can use Gensim - a Python library for Topic Modeling. You can install Gensim by using conda or pip. See [https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)\n",
    "\n",
    "```\n",
    "conda install gensim\n",
    "```\n",
    "\n",
    "After install gensim, we can start using the library for topic modeling.\n",
    "\n",
    "#### Reading data in LDA-C format\n",
    "\n",
    "We use class BleiCorpus in the module ```gensim.corpora.bleicorpus``` to load the data in the file ```ap.dat``` and vocabulary file ```vocab.txt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "corpus = gensim.corpora.BleiCorpus('./ap/ap.dat', './ap/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the corpus in a loop like so:\n",
    "\n",
    "```\n",
    "for document in corpus:\n",
    "    # Some process on the document\n",
    "```\n",
    "\n",
    "If we just want to look at the content of the first document, we can do as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (6144, 1.0), (3586, 2.0), (3, 1.0), (4, 1.0), (1541, 1.0), (8, 1.0), (10, 1.0), (3927, 1.0), (12, 7.0), (4621, 1.0), (527, 1.0), (9232, 1.0), (1112, 2.0), (20, 1.0), (2587, 1.0), (6172, 1.0), (10269, 2.0), (37, 1.0), (42, 1.0), (3117, 1.0), (1582, 1.0), (1585, 3.0), (435, 1.0), (9268, 3.0), (571, 2.0), (60, 1.0), (61, 1.0), (63, 2.0), (64, 2.0), (5185, 1.0), (11, 1.0), (4683, 1.0), (590, 2.0), (1103, 2.0), (592, 1.0), (5718, 1.0), (1623, 2.0), (1624, 4.0), (89, 2.0), (6234, 1.0), (8802, 1.0), (1638, 1.0), (103, 1.0), (600, 1.0), (9404, 1.0), (106, 1.0), (3691, 1.0), (720, 1.0), (2672, 1.0), (113, 1.0), (2165, 1.0), (5751, 1.0), (123, 3.0), (1148, 1.0), (128, 2.0), (1670, 2.0), (4231, 1.0), (1167, 1.0), (144, 1.0), (147, 1.0), (149, 7.0), (3735, 2.0), (5272, 2.0), (1732, 1.0), (673, 2.0), (5282, 1.0), (27, 1.0), (1700, 1.0), (9893, 2.0), (166, 1.0), (167, 1.0), (173, 1.0), (174, 1.0), (2224, 1.0), (2248, 1.0), (372, 2.0), (186, 1.0), (4284, 3.0), (3450, 2.0), (117, 2.0), (203, 1.0), (2244, 1.0), (5320, 1.0), (201, 1.0), (4215, 1.0), (9932, 2.0), (207, 2.0), (208, 5.0), (8914, 1.0), (7898, 1.0), (733, 2.0), (1760, 1.0), (1744, 1.0), (744, 1.0), (234, 1.0), (1259, 2.0), (4287, 1.0), (7254, 1.0), (249, 1.0), (8311, 1.0), (5884, 2.0), (298, 1.0), (254, 1.0), (767, 2.0), (2304, 1.0), (4876, 1.0), (270, 1.0), (557, 1.0), (786, 1.0), (789, 2.0), (2331, 1.0), (287, 1.0), (5409, 1.0), (290, 1.0), (5923, 1.0), (2854, 1.0), (1834, 3.0), (303, 1.0), (3888, 4.0), (817, 2.0), (9523, 1.0), (334, 1.0), (1333, 1.0), (311, 2.0), (1855, 1.0), (1417, 1.0), (325, 1.0), (1870, 7.0), (1361, 1.0), (1362, 1.0), (6995, 1.0), (342, 1.0), (343, 1.0), (344, 1.0), (857, 1.0), (5469, 2.0), (351, 5.0), (1377, 1.0), (2402, 1.0), (487, 1.0), (884, 1.0), (885, 1.0), (890, 1.0), (4477, 1.0), (3455, 1.0), (1410, 1.0), (5099, 1.0), (4489, 1.0), (395, 1.0), (2570, 1.0), (152, 1.0), (404, 1.0), (1429, 1.0), (1430, 1.0), (3992, 1.0), (416, 1.0), (3491, 1.0), (2033, 1.0), (3499, 1.0), (429, 1.0), (3502, 1.0), (5040, 1.0), (433, 2.0), (1971, 4.0), (437, 1.0), (9667, 2.0), (322, 1.0), (7119, 1.0), (8656, 1.0), (1102, 1.0), (985, 1.0), (989, 1.0), (1840, 1.0), (2529, 1.0), (997, 1.0), (2022, 2.0), (4071, 1.0), (2536, 1.0), (10219, 1.0), (1517, 1.0), (1009, 1.0), (221, 1.0), (3059, 1.0), (500, 1.0), (511, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print( next(iter(corpus)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair printed in the above output is the pair of termID:termCount. ```termID``` is the word index in the vocabulary and ```termCount``` is the number of times term ```termID``` occurs in the document.\n",
    "\n",
    "We can identify words that correspond to term indexes. Here we just print 10 words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 1.0\n",
      "maurice 1.0\n",
      "adult 2.0\n",
      "people 1.0\n",
      "year 1.0\n",
      "h 1.0\n",
      "last 1.0\n",
      "years 1.0\n",
      "resolved 1.0\n",
      "police 7.0\n"
     ]
    }
   ],
   "source": [
    "doc = next(iter(corpus))\n",
    "for word_id, freq in doc[0:10]:\n",
    "    print(corpus.id2word[word_id], freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating topic models from the data\n",
    "\n",
    "Now we will train a LDA model on the data we have and save the model to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(corpus, id2word=corpus.id2word, \n",
    "                               alpha='auto', num_topics=100)\n",
    "model.save('ap.lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the tutorial [Using Gensim for LDA](http://christop.club/2014/05/06/using-gensim-for-lda/), the meaning of parameters we used in the above function is as follows. (We just copy & paste here)\n",
    "\n",
    "1. id2word: Although you can build a model from just a corpus, Iâ€™ve gone ahead and let the LdaModel know about the corpus.id2word. It just makes some of the things Iâ€™ll show you next nicer.\n",
    "2. alpha: This particular LDA implementation uses something that can automatically update the alpha value for us. This determines how â€˜smoothâ€™ the model is, which makes no damned sense if you arenâ€™t working in the area (it doesnâ€™t make much sense to me). Hereâ€™s what alpha does: as it gets smaller, each document is going to be more specific, i.e., likely to only made up of a few topics. As it gets bigger, a document can begin to appear in multiple topics, which is what we want. Itâ€™s not good to have a large alpha either, because then all our topics will start intermingling and making out and thatâ€™s gross.\n",
    "3. num_topics: The num_topics parameter just determines how many topics we want the model to give us. Iâ€™ve used 100 here since we are only looking at a corpus of titles.\n",
    "\n",
    "Now let's look at some random topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(79,\n",
       "  '0.087*cdy + 0.078*clr + 0.027*rn + 0.012*eduardo + 0.011*uprisings + 0.010*havana + 0.008*menem + 0.005*new + 0.005*two + 0.004*ali'),\n",
       " (87,\n",
       "  '0.013*yeutter + 0.010*fresh + 0.008*outlets + 0.008*deteriorating + 0.008*forestry + 0.008*iran + 0.006*states + 0.006*perez + 0.005*trade + 0.004*united'),\n",
       " (32,\n",
       "  '0.008*year + 0.006*i + 0.005*million + 0.005*two + 0.005*last + 0.005*percent + 0.004*first + 0.004*new + 0.004*people + 0.004*years'),\n",
       " (34,\n",
       "  '0.012*i + 0.012*united + 0.010*states + 0.008*year + 0.004*people + 0.004*going + 0.004*years + 0.004*first + 0.004*congress + 0.004*new'),\n",
       " (82,\n",
       "  '0.009*billion + 0.008*trade + 0.007*united + 0.007*states + 0.005*last + 0.005*canadian + 0.005*percent + 0.005*imports + 0.004*japan + 0.004*world')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=5, num_words=10, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get top 5 topics and words with probabilities that words belong the topic.\n",
    "\n",
    "Now let's do querying the trained topic model with a *fake* query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['police', 'gun', 'boy', 'minister', 'government']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"police gun boy minister government\"\n",
    "query = query.split()\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dictionary that is a map from word id to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.VocabTransform at 0x10ef2fcf8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word = gensim.corpora.Dictionary()\n",
    "id2word.merge_with(corpus.id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we converty the query to the list of tuples (word, frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1), (12, 1), (122, 1), (1585, 1), (1624, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = id2word.doc2bow(query)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we infer the distribution over topics for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42, 0.40112170293508975), (75, 0.42665021287633781)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1000)\n",
    "a = list(sorted(model[query], key=lambda x: x[1]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s check out what words are most associated with those some topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.014*government + 0.013*party + 0.009*police + 0.007*mexican + 0.007*allied + 0.006*threats + 0.005*mexico + 0.005*national + 0.005*two + 0.005*minister'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(a[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.014*government + 0.013*party + 0.009*police + 0.007*mexican + 0.007*allied + 0.006*threats + 0.005*mexico + 0.005*national + 0.005*two + 0.005*minister'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topic(a[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Implementation of LDA\n",
    "\n",
    "There is also an implementation of LDA for R language. Please see the package implemented by Jonathan Chang on [lda: Collapsed Gibbs Sampling Methods for Topic Models](https://cran.r-project.org/web/packages/lda/index.html).\n",
    "\n",
    "## Applications of Topic Modeling in Text Classification and Text Clustering\n",
    "\n",
    "In this section, we will learn how we can use topic distributions of documents as new representations for the text classification task and compare with the **BOW** representation.\n",
    "\n",
    "We will use [the 20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/) for this tutorial. The data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "\n",
    "### Basic Idea\n",
    "\n",
    "The basic idea is to train a topic model on the training portion of the data. We will get the distribution over words for documents in the training data. We use the new representations to train a classification model on the training data. In testing, we used the generated topic model to infer topic distributions, and used the trained classifier to predict the labels for documents in test data.\n",
    "\n",
    "### Data Set\n",
    "\n",
    "We will use the python library [scikit-learn](http://scikit-learn.org/stable/about.html) to fetch 20newsgroups data. Since it takes time to train LDA model from all documents in the data, we just use a subset of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', data_home = './',\n",
    "                                categories = categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', data_home = './',\n",
    "                               categories = categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now list all newsgroups in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "print(target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you may to know some basic statistics about data, such as the number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2034\n",
      "Number of test examples: 1353\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples: %d' % len(data_train.data))\n",
    "print('Number of test examples: %d' % len(data_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with BOW features\n",
    "\n",
    "In this section, we will perform text classification on 20newsgroups data by using Bag-of-word (BOW) features. The section is based on [the tutorial](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html) on scikit-learn.\n",
    "\n",
    "We use TF-IDF weighting scheme to represent documents in the data set. We use [Support Vector Machine (SVM)](https://en.wikipedia.org/wiki/Support_vector_machine) as the classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "n_samples: 2034, n_features: 26576\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "n_samples: 1353, n_features: 26576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', data_home = './',\n",
    "                                categories = categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', data_home = './',\n",
    "                               categories = categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "\n",
    "# sublinear_tf : boolean, default=False\n",
    "# Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train a classification model using SVM with linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "accuracy:   0.780\n",
      "classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.69      0.62      0.66       319\n",
      "     comp.graphics       0.89      0.91      0.90       389\n",
      "         sci.space       0.78      0.90      0.84       394\n",
      "talk.religion.misc       0.68      0.60      0.64       251\n",
      "\n",
      "       avg / total       0.77      0.78      0.78      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "print(target_names)\n",
    "clf = LinearSVC(loss='squared_hinge', penalty='l2', dual=False, tol=1e-3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_test, pred,\n",
    "                                    target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We now estimate topic models from the training portion of the data set. We will ```gensim``` to do the task.\n",
    "\n",
    "We follow the tutorial [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html), and go through step by step to create a LDA model from the training data.\n",
    "\n",
    "We will do following steps in preprocessing:\n",
    "\n",
    "- Tokenization\n",
    "- Removing stop words\n",
    "- Stemming\n",
    "\n",
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "docs_train = [word_tokenize(doc) for doc in data_train.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "Now, we remove stop words in the training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "english_stops = set(stopwords.words('english'))\n",
    "punc_set = set( [ c for c in string.punctuation ] )\n",
    "\n",
    "def stopwords_and_punc_filter(doc):\n",
    "    return [word.lower() for word in doc \n",
    "            if (word.lower() not in english_stops) and\n",
    "            (word not in punc_set) and \n",
    "            (re.search(r'^[0-9a-zA-Z_]+$', word))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function ```stopwords_and_punc_filter``` to remove stop words in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'noticed', 'save', 'model', 'mapping', 'planes', 'positioned', 'carefully', 'file', 'reload', 'restarting', '3ds', 'given', 'default', 'position', 'orientation', 'save', 'file', 'preserved', 'anyone', 'know', 'information', 'stored', 'file', 'nothing', 'explicitly', 'said', 'manual', 'saving', 'texture', 'rules', 'file', 'like', 'able', 'read', 'texture', 'rule', 'information', 'anyone', 'format', 'file', 'file', 'format', 'available', 'somewhere', 'rych']\n"
     ]
    }
   ],
   "source": [
    "docs_train_no_stopwords = [ stopwords_and_punc_filter(doc) \n",
    "                            for doc in docs_train ]\n",
    "print(docs_train_no_stopwords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "We define a function for stemming a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "def stemming_doc(doc):\n",
    "    return [porter.stem(t) for t in doc]\n",
    "\n",
    "def lemmatization(doc):\n",
    "    return [wnl.lemmatize(t) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above function for stemming step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'noticed', 'save', 'model', 'mapping', 'plane', 'positioned', 'carefully', 'file', 'reload', 'restarting', '3d', 'given', 'default', 'position', 'orientation', 'save', 'file', 'preserved', 'anyone', 'know', 'information', 'stored', 'file', 'nothing', 'explicitly', 'said', 'manual', 'saving', 'texture', 'rule', 'file', 'like', 'able', 'read', 'texture', 'rule', 'information', 'anyone', 'format', 'file', 'file', 'format', 'available', 'somewhere', 'rych']\n"
     ]
    }
   ],
   "source": [
    "# train_texts = [stemming_doc(doc) for doc in docs_train_no_stopwords]\n",
    "# train_texts = docs_train_no_stopwords\n",
    "train_texts = [lemmatization(doc) for doc in docs_train_no_stopwords]\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Estimating Topic Models\n",
    "\n",
    "We will create a dictionary from the collection of training documents.\n",
    "\n",
    "The ```Dictionary()``` function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics. To see each tokenâ€™s unique integer id, try ```print(dictionary.token2id)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(train_texts)\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our dictionary must be converted into a ```bag-of-words```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 2), (17, 2), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 6), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 2), (33, 1), (34, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having term-document matrix, we are now ready to build LDA model. In order to speed up estimating LDA models, we use the module ```LdaMulticore```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.ldamulticore.LdaMulticore(corpus, \n",
    "                                                id2word=dictionary, \n",
    "                                                num_topics=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the results\n",
    "\n",
    "Our LDA model is now stored as ldamodel. We can review our topics with the ```print_topic``` and ```print_topics``` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(498, '0.006*new + 0.006*washington + 0.006*street + 0.005*space + 0.005*time + 0.005*york + 0.005*source + 0.004*also + 0.004*dc + 0.004*ny'), (132, '0.015*order + 0.009*reuss + 0.007*image + 0.007*u + 0.006*texas + 0.006*post + 0.005*member + 0.005*omniscient + 0.005*use + 0.005*whole')]\n"
     ]
    }
   ],
   "source": [
    "print(model.print_topics(num_topics=2, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Topic Distributions of Documents for Classification\n",
    "\n",
    "Now we will use distribution over topics for the classification task. We need to use LDA model to infer topic distributions of documents in training and test data.\n",
    "\n",
    "Let's infer topic distributions for training documents. For instance, we get the topic distribution for document 0 in the corpus by using following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4.2553191489361501e-05),\n",
       " (1, 4.2553191489361501e-05),\n",
       " (2, 4.2553191489361501e-05),\n",
       " (3, 4.2553191489361501e-05),\n",
       " (4, 4.2553191489361501e-05),\n",
       " (5, 4.2553191489361501e-05),\n",
       " (6, 4.2553191489361501e-05),\n",
       " (7, 4.2553191489361501e-05),\n",
       " (8, 4.2553191489361501e-05),\n",
       " (9, 4.2553191489361501e-05)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_doc = model.__getitem__(corpus[0], 0.00001)\n",
    "# Print some topics with probabilities\n",
    "lda_doc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get topic distributions for all training documents, and convert them to feature vectors. These feature vectors will be used as input of Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "lda_docs_train = [model.__getitem__(doc, 0.00001) for doc in corpus]\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X_lda_train = vectorizer.fit_transform([dict(doc) for doc in lda_docs_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to infer topic distributions for all documents in test data. We walk through similar steps as for training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "docs_test = [word_tokenize(doc) for doc in data_test.data]\n",
    "docs_test_no_stopwords = [ stopwords_and_punc_filter(doc) \n",
    "                            for doc in docs_test ]\n",
    "test_texts  = [lemmatization(doc) for doc in docs_test_no_stopwords]\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We infer topics distributions for test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_docs_test = [model.__getitem__(doc, 0.00001) for doc in test_corpus]\n",
    "lda_docs_test[0][0:10]\n",
    "X_lda_test = vectorizer.fit_transform([dict(doc) for doc in lda_docs_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the data with new representations of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.607\n",
      "classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       alt.atheism       0.58      0.45      0.51       319\n",
      "     comp.graphics       0.65      0.77      0.70       389\n",
      "         sci.space       0.60      0.75      0.66       394\n",
      "talk.religion.misc       0.56      0.32      0.41       251\n",
      "\n",
      "       avg / total       0.60      0.61      0.59      1353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(loss='squared_hinge', penalty='l2', dual=False, tol=1e-3)\n",
    "clf.fit(X_lda_train, y_train)\n",
    "\n",
    "pred = clf.predict(X_lda_test)\n",
    "\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "print(\"classification report:\")\n",
    "print(metrics.classification_report(y_test, pred,\n",
    "                                    target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is much lower than the using BOW representations. In order to improve the result, you may want to try:\n",
    "\n",
    "- Tunning the parameters for Topic Models (i.e., the number of topics, alpha, etc)\n",
    "- Applying feature selection\n",
    "- Combine BOW representations with topical distribution-based features\n",
    "\n",
    "**Three possible above methods are left as practice exercises for you.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering with BOW Representations\n",
    "\n",
    "We know apply k-means algorithm for text clustering on the 20newsgroups data set. In this section, we use BOW representations of documents for text clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 10.239s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "labels = data_train.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', random_state=42, verbose=False)\n",
    "t0 = time()\n",
    "km.fit(X_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.278\n",
      "Completeness: 0.365\n",
      "V-measure: 0.316\n",
      "Adjusted Rand-Index: 0.234\n",
      "Silhouette Coefficient: 0.007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_train, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering with Topic Distributions\n",
    "\n",
    "We use topic distributions of documents as new representations of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.212s\n",
      "\n",
      "Homogeneity: 0.003\n",
      "Completeness: 0.063\n",
      "V-measure: 0.006\n",
      "Adjusted Rand-Index: 0.001\n",
      "Silhouette Coefficient: 0.006\n",
      "\n"
     ]
    }
   ],
   "source": [
    "km_lda = KMeans(n_clusters=true_k, init='k-means++', random_state=42, verbose=False)\n",
    "t0 = time()\n",
    "km.fit(X_lda_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_train, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that comparing with BOW representations, results of using topic distribution are much worse. **Please think how to improve these results**.\n",
    "\n",
    "One possible improvement is to normalizing data before clustering. **We leave this as an exercise for readers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## References\n",
    "\n",
    "- [Using Gensim for LDA](http://christop.club/2014/05/06/using-gensim-for-lda/)\n",
    "- [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)\n",
    "- [models.ldamodel â€“ Latent Dirichlet Allocation](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "- [Classification of text documents using sparse features](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html)\n",
    "- [The 20 newsgroups text datas](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- [Clustering on scikit-learn](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [Clustering text documents using k-means](http://scikit-learn.org/stable/auto_examples/text/document_clustering.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
