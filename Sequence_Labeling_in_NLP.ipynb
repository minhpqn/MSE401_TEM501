{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence models in natural language processing\n",
    "\n",
    "Text mining course\n",
    "\n",
    "## Outline\n",
    "\n",
    "This document contains following contents.\n",
    "\n",
    "- How to estimate parameters in a first-order HMM tagger\n",
    "- How to use nltk.tag.hmm to train a first-order HMM tagger\n",
    "- How to use CRF++ toolkit for POS tagging\n",
    "\n",
    "## Implementation of a first-order HMM tagger\n",
    "\n",
    "In a HMM, we need to to estimate emission and transition probabilties from a training data set. We will use the training data in the file `wikientrain.norm_pos`.\n",
    "\n",
    "The file contains sentences whose words are associated with POS tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path in your environment\n",
    "TRAINING_DATA = \"./data/POS/wiki-en-train.norm_pos\"\n",
    "TEST_DATA = \"./data/POS/wiki-en-test.norm_pos\"\n",
    "TEST_RAW_DATA = \"./data/POS/wiki-en-test.norm\"\n",
    "GOLD_TEST_DATA = \"./data/POS/wiki-en-test.pos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data from the file into a list of strings and look at some first lines in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(TRAINING_DATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip() # strip trailing spaces and new-line char\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        lines.append(line)\n",
    "print(\"# Loaded {} lines\".format(len(lines)))\n",
    "# Print first 5 lines\n",
    "for i in range(5):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the train file is a list of words with their POS tags. We will use that data to train an HMM tagger.\n",
    "\n",
    "### Parameter estimation\n",
    "\n",
    "Transition probabilities in a first-order HMM tagger are calculated as follows.\n",
    "\n",
    "$$\n",
    "P(t_i|t_{i-1})=\\frac{C(t_{i-1},t_i)}{C(t_{i-1})}\n",
    "$$\n",
    "\n",
    "Here $C(t_{i-1},t_i)$ is the number of times the tag $t_i$ follows the tag $t_{i-1}$ in the corpus and $C(t_{i-1})$ is the number of times tag $t_{i-1}$ occurs in the corpus.\n",
    "\n",
    "Emission probabilities are calculated as follows\n",
    "\n",
    "$$\n",
    "P(w_i|t_i)=\\frac{C(t_i,w_i)}{C(t_i)}\n",
    "$$\n",
    "\n",
    "$C(t_i,w_i)$ is the number of times the word $w_i$ is associated with the tag $t_i$\n",
    "\n",
    "Below is the implementation of transition probability estimation. Students need to implement emission probabilities by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def train_hmm():\n",
    "    transition = {}  # Transition probabilities\n",
    "    emis = {} # Emission probabilities that students need to estimate\n",
    "    transition_count = defaultdict(int)  # Transition counts \n",
    "    context = defaultdict(int)  # Count of the context\n",
    "    \n",
    "    with open(TRAINING_DATA, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            previous = \"<s>\"  # Make the sentence start\n",
    "            context[previous] += 1\n",
    "            wordtags = line.split()\n",
    "            for wordtag in wordtags:\n",
    "                word, tag = wordtag.split(\"_\")\n",
    "                transition_count[(previous + \" \" + tag)] += 1\n",
    "                context[tag] += 1\n",
    "                previous = tag\n",
    "            transition_count[previous + \" \" + \"</s>\"] += 1\n",
    "    # Calculate transition probabilities and print out\n",
    "    # In real program, you should not print probabilities out\n",
    "    for key, count in transition_count.items():\n",
    "        previous, tag = key.split(\" \")\n",
    "        transition[key] = count/context[previous]\n",
    "        print(\"T {} {}\".format(key, transition[key]))\n",
    "    # Calculate emission probabilities\n",
    "    # Save probabilities into a model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hmm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training HMM tagger, you need to save transition and emission probabilities into a model file using above format. You may want to use \"E\" for marking emission probabilties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi algorithm\n",
    "\n",
    "Implementation of Viterbi algorithm in HMM tagger is left as the assignment for students.\n",
    "\n",
    "Students need to define following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hmm():\n",
    "    # Implementation of Viterbi algorithm\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk.tag.hmm for training a POS tagger\n",
    "\n",
    "In this section, we will use the module `nltk.tag.hmm` to train a POS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data into list of sentences. Each sentence is a list of tuples (word, pos)\n",
    "train_data = []\n",
    "\n",
    "def load_data(input_file):    \n",
    "    data = []\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue\n",
    "            wordtags = line.split()\n",
    "            sentence = []\n",
    "            for wordtag in wordtags:\n",
    "                word, tag = wordtag.split(\"_\")\n",
    "                sentence.append((word,tag))\n",
    "            data.append(sentence)\n",
    "    return data\n",
    "\n",
    "train_data = load_data(TRAINING_DATA)\n",
    "print(\"# Loaded {} sentences\".format(len(train_data)))\n",
    "print(\"# Show the first sentence\")\n",
    "print(train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next will will use the training data to train a HMM tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the toolkit and tags\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Setup a trainer with default(None) values\n",
    "# And train with the data\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(train_data)\n",
    "print(tagger)\n",
    "\n",
    "gold_sentences = load_data(TEST_DATA)\n",
    "print(\"# Test accuracy: {}\".format(tagger.evaluate(gold_sentences)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the trained tagger to tag some sentences in the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.tag(\"The bass line of the song is too weak .\".split()))\n",
    "\n",
    "print(tagger.tag(\"Still , supervised systems continue to perform best .\".split()))\n",
    "\n",
    "print(tagger.tag(\"A set of testing words .\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the trained HMM tagger to tag sentences in the test data and write the output to a file says `nltk_answer.pos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"./nltk_answer.pos\"\n",
    "fo = open(output_file, \"w\")\n",
    "with open(TEST_RAW_DATA, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        tagged_sen = tagger.tag(line.split())\n",
    "        tags = [ tag for _, tag in tagged_sen]\n",
    "        fo.write(\"{}\\n\".format(\" \".join(tags)))\n",
    "fo.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the perl script to evaluate the result of the tagger trained by `nltk.tag.hmm`\n",
    "\n",
    "    ./gradepos.pl ./data/POS/wiki-en-test.pos ./nltk_answer.pos\n",
    "\n",
    "It turned out that the trained hmm tagger is not good.\n",
    "\n",
    "```\n",
    "Accuracy: 35.26% (1609/4563)\n",
    "\n",
    "Most common mistakes:\n",
    "NN --> JJ\t607\n",
    "IN --> JJ\t358\n",
    "NNS --> JJ\t281\n",
    "DT --> JJ\t250\n",
    ". --> JJ\t152\n",
    ", --> JJ\t152\n",
    "RB --> JJ\t125\n",
    "VBN --> JJ\t115\n",
    "CC --> JJ\t107\n",
    "VB --> JJ\t80\n",
    "```\n",
    "\n",
    "You can customize the HMM tagger by using options in training. See the the source code of `nltk.tag.hmm` in [https://www.nltk.org/_modules/nltk/tag/hmm.html](https://www.nltk.org/_modules/nltk/tag/hmm.html) and the documentation in [https://www.nltk.org/api/nltk.tag.html?highlight=tag%20hmm#module-nltk.tag.hmm](https://www.nltk.org/api/nltk.tag.html?highlight=tag%20hmm#module-nltk.tag.hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use nltk.tag.crf for training an POS tagger\n",
    "\n",
    "This section will use `nltk.tag.crf` module to train and test an POS tagger\n",
    "\n",
    "Reference: [https://www.nltk.org/_modules/nltk/tag/crf.html](https://www.nltk.org/_modules/nltk/tag/crf.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "crf_model_file = \"model.crf.tagger\"\n",
    "ct = CRFTagger()\n",
    "ct.train(train_data, crf_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we learn how to load the crf model file and evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "ct = CRFTagger()\n",
    "crf_model_file = \"model.crf.tagger\"\n",
    "ct.set_model_file(crf_model_file)\n",
    "\n",
    "gold_sentences = load_data(TEST_DATA)\n",
    "print(\"# Test accuracy: {}\".format(ct.evaluate(gold_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained much higher accuracy than the HMM tagger trained by using `nltk.tag.hmm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CRF++ for POS tagging\n",
    "\n",
    "In this section, we will learn how to use CRF++ ([https://taku910.github.io/crfpp/](https://taku910.github.io/crfpp/)) for POS tagging.\n",
    "\n",
    "You should download and install CRF++ first.\n",
    "\n",
    "### Building training/test file for CRF++\n",
    "\n",
    "We convert data files into format that is used by CRF++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(input_file, output_file):\n",
    "    with open(output_file, \"w\") as fo:\n",
    "        with open(input_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    continue\n",
    "                wordtags = line.split()\n",
    "                for wordtag in wordtags:\n",
    "                    word, tag = wordtag.split(\"_\")\n",
    "                    fo.write(\"{} {}\\n\".format(word, tag))\n",
    "                fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use above functions to make CRF++ training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_train = \"./train_crf.txt\"\n",
    "crf_test = \"./test_crf.txt\"\n",
    "\n",
    "convert_data(TRAINING_DATA, crf_train)\n",
    "\n",
    "convert_data(TEST_DATA, crf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a CRF model, we need to create a template file. We already prepared the template file for POS tagging task as follows.\n",
    "\n",
    "```\n",
    "# Unigram\n",
    "U00:%x[-2,0]\n",
    "U01:%x[-1,0]\n",
    "U02:%x[0,0]\n",
    "U03:%x[1,0]\n",
    "U04:%x[2,0]\n",
    "U05:%x[-1,0]/%x[0,0]\n",
    "U06:%x[0,0]/%x[1,0]\n",
    "\n",
    "# Bigram\n",
    "B\n",
    "```\n",
    "\n",
    "Now we train CRF++ model by using following command line\n",
    "\n",
    "    crf_learn -p 2 template train_crf.txt crfpp_model\n",
    "\n",
    "Now test on the test data\n",
    "\n",
    "    crf_test -m crfpp_model ./test_crf.txt > ./test_crf.out.txt\n",
    "\n",
    "We convert back to POS file for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "crf_output = \"./test_crfpp.pos\"\n",
    "\n",
    "def write_pos_file(pos_output, crf_output):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    with open(crf_output, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Blank line\n",
    "            if re.search(r\"^[\\s\\t\\n]*$\", line):\n",
    "                if len(sentence) > 0:\n",
    "                    data.append(sentence)\n",
    "                    sentence = []\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                line = line.strip()\n",
    "                word, tag, predicted_tag = line.split()\n",
    "                sentence.append(predicted_tag)\n",
    "    with open(pos_output, \"w\") as fo:\n",
    "        for sentence in data:\n",
    "            fo.write(\"{}\\n\".format(\" \".join(sentence)))\n",
    "\n",
    "write_pos_file(crf_output, \"./test_crf.out.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the perl script `gradepos.pl` for evaluation.\n",
    "\n",
    "     ./gradepos.pl ./data/POS/wiki-en-test.pos ./test_crfpp.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this document, we have learned:\n",
    "\n",
    "- How to estimate parameters for an HMM tagger\n",
    "- Using `nltk.tag.hmm` and `ntlk.tag.crf` for building POS taggers\n",
    "- Using CRF++ for building CRF POS tagger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
